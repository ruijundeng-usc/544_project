{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StoryGenerator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsvtnnHaRwyB"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOFeGtCYikdk",
        "outputId": "e2d3357b-0d04-4a1d-973d-1979db62dc18"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq2tURa8R72Q"
      },
      "source": [
        "#need to load data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/ROCStories_winter2017 - ROCStories_winter2017.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ph3V-G4U0d3",
        "outputId": "9da22eef-304b-401a-dd1c-a1cbff431a06"
      },
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                storyid               storytitle  \\\n",
            "0  8bbe6d11-1e2e-413c-bf81-eaea05f4f1bd   David Drops the Weight   \n",
            "1  0beabab2-fb49-460e-a6e6-f35a202e3348              Frustration   \n",
            "2  87da1a22-df0b-410c-b186-439700b70ba6       Marcus Buys Khakis   \n",
            "3  2d16bcd6-692a-4fc0-8e7c-4a6f81d9efa9       Different Opinions   \n",
            "4  c71bb23b-7731-4233-8298-76ba6886cee1  Overcoming shortcomings   \n",
            "\n",
            "                                           sentence1  \\\n",
            "0  David noticed he had put on a lot of weight re...   \n",
            "1                       Tom had a very short temper.   \n",
            "2  Marcus needed clothing for a business casual e...   \n",
            "3  Bobby thought Bill should buy a trailer and ha...   \n",
            "4          John was a pastor with a very bad memory.   \n",
            "\n",
            "                                           sentence2  \\\n",
            "0  He examined his habits to try and figure out t...   \n",
            "1               One day a guest made him very angry.   \n",
            "2  All of his clothes were either too formal or t...   \n",
            "3  Bill thought a truck would be better for what ...   \n",
            "4  He tried to memorize his sermons many days in ...   \n",
            "\n",
            "                                           sentence3  \\\n",
            "0  He realized he'd been eating too much fast foo...   \n",
            "1        He punched a hole in the wall of his house.   \n",
            "2                He decided to buy a pair of khakis.   \n",
            "3  Bobby pointed out two vehicles were much more ...   \n",
            "4  He decided to learn to sing to overcome his ha...   \n",
            "\n",
            "                                           sentence4  \\\n",
            "0  He stopped going to burger places and started ...   \n",
            "1        Tom's guest became afraid and left quickly.   \n",
            "2              The pair he bought fit him perfectly.   \n",
            "3  Bill was set in his ways with conventional thi...   \n",
            "4  He then made all his sermons into music and sa...   \n",
            "\n",
            "                                           sentence5  \n",
            "0  After a few weeks, he started to feel much bet...  \n",
            "1  Tom sat on his couch filled with regret about ...  \n",
            "2  Marcus was happy to have the right clothes for...  \n",
            "3  He ended up buying the truck he wanted despite...  \n",
            "4      His congregation was delighted and so was he.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwJ0uLomj68-"
      },
      "source": [
        "## Storyline Planning (static)\n",
        "Use BiLSTM for encoding and LSTM for decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSNmf-21T7fZ",
        "outputId": "1b768fd6-a274-42b8-ccef-967318cb9b3e"
      },
      "source": [
        "!pip install rake-nltk\n",
        "from rake_nltk import Rake\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Uses stopwords for english from NLTK, and all punctuation characters by\n",
        "# default\n",
        "#max_length=1 so that we only get one word\n",
        "r = Rake(max_length=1, min_length=1, include_repeated_phrases=False)\n",
        "\n",
        "def rake_implement(s1, s2, s3, s4, s5, r):\n",
        "    result = []\n",
        "    result.append(get_word(r, s1))\n",
        "    result.append(get_word(r, s2))\n",
        "    result.append(get_word(r, s3))\n",
        "    result.append(get_word(r, s4))\n",
        "    result.append(get_word(r, s5))\n",
        "    return result\n",
        "\n",
        "def get_word(r, s):\n",
        "  r.extract_keywords_from_text(s)\n",
        "  phrases = r.get_ranked_phrases()\n",
        "  if(len(phrases) == 0):\n",
        "    #if nothing is extracted, take first word that is not in stopwords (from the middle)\n",
        "    sentence = s.split()\n",
        "    for i in range(int(len(sentence)/2), len(sentence)):\n",
        "      if not sentence[i] in stopwords.words('english'):\n",
        "        return sentence[i].lower()\n",
        "  else:\n",
        "    return phrases[0]\n",
        "\n",
        "#https://stackoverflow.com/questions/56836477/apply-nltk-rake-to-each-row-in-dataframe\n",
        "df['train_storylines'] = df.apply(lambda x: rake_implement(x.sentence1, x.sentence2, x.sentence3, x.sentence4, x.sentence5, r), axis=1)\n",
        "\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rake-nltk\n",
            "  Downloading rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n",
            "Collecting nltk<4.0.0,>=3.6.2\n",
            "  Downloading nltk-3.6.5-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.62.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (7.1.2)\n",
            "Collecting regex>=2021.8.3\n",
            "  Downloading regex-2021.11.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 21.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: regex, nltk, rake-nltk\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.5 rake-nltk-1.0.6 regex-2021.11.10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                storyid               storytitle  \\\n",
            "0  8bbe6d11-1e2e-413c-bf81-eaea05f4f1bd   David Drops the Weight   \n",
            "1  0beabab2-fb49-460e-a6e6-f35a202e3348              Frustration   \n",
            "2  87da1a22-df0b-410c-b186-439700b70ba6       Marcus Buys Khakis   \n",
            "3  2d16bcd6-692a-4fc0-8e7c-4a6f81d9efa9       Different Opinions   \n",
            "4  c71bb23b-7731-4233-8298-76ba6886cee1  Overcoming shortcomings   \n",
            "\n",
            "                                           sentence1  \\\n",
            "0  David noticed he had put on a lot of weight re...   \n",
            "1                       Tom had a very short temper.   \n",
            "2  Marcus needed clothing for a business casual e...   \n",
            "3  Bobby thought Bill should buy a trailer and ha...   \n",
            "4          John was a pastor with a very bad memory.   \n",
            "\n",
            "                                           sentence2  \\\n",
            "0  He examined his habits to try and figure out t...   \n",
            "1               One day a guest made him very angry.   \n",
            "2  All of his clothes were either too formal or t...   \n",
            "3  Bill thought a truck would be better for what ...   \n",
            "4  He tried to memorize his sermons many days in ...   \n",
            "\n",
            "                                           sentence3  \\\n",
            "0  He realized he'd been eating too much fast foo...   \n",
            "1        He punched a hole in the wall of his house.   \n",
            "2                He decided to buy a pair of khakis.   \n",
            "3  Bobby pointed out two vehicles were much more ...   \n",
            "4  He decided to learn to sing to overcome his ha...   \n",
            "\n",
            "                                           sentence4  \\\n",
            "0  He stopped going to burger places and started ...   \n",
            "1        Tom's guest became afraid and left quickly.   \n",
            "2              The pair he bought fit him perfectly.   \n",
            "3  Bill was set in his ways with conventional thi...   \n",
            "4  He then made all his sermons into music and sa...   \n",
            "\n",
            "                                           sentence5  \\\n",
            "0  After a few weeks, he started to feel much bet...   \n",
            "1  Tom sat on his couch filled with regret about ...   \n",
            "2  Marcus was happy to have the right clothes for...   \n",
            "3  He ended up buying the truck he wanted despite...   \n",
            "4      His congregation was delighted and so was he.   \n",
            "\n",
            "                              train_storylines  \n",
            "0         [put, try, realized, started, weeks]  \n",
            "1              [tom, angry, wall, tom, regret]  \n",
            "2  [business, formal, pair, perfectly, marcus]  \n",
            "3         [trailer, needed, much, ways, truck]  \n",
            "4    [pastor, tried, sing, sundays, delighted]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4m86rabiDDD"
      },
      "source": [
        "#to convert words to indices and vice versa\n",
        "#https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "class Vocab:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {\"<SOS>\": 0, \"<EOT>\": 1, \"<EOS>\": 2}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"<SOS>\", 1:\"<EOT>\", 2:\"<EOS>\"}\n",
        "        self.n_words = 3\n",
        "    \n",
        "    def addTitle(self, title):\n",
        "      for word in title.split():\n",
        "        self.addWordToCount(word)\n",
        "\n",
        "    def addStoryline(self, storyline):\n",
        "        for word in storyline:\n",
        "            #could do preprocessing here\n",
        "            self.addWordToCount(word)\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "      for word in sentence.split():\n",
        "        self.addWordToCount(word)\n",
        "\n",
        "    def addWordToCount(self, word):\n",
        "        if word not in self.word2count:\n",
        "            self.word2count[word] = 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def addWordToDicts(self, word):\n",
        "      self.word2index[word] = self.n_words\n",
        "      self.index2word[self.n_words] = word\n",
        "      self.n_words += 1\n",
        "\n",
        "    def removeWordFromCount(self, word):\n",
        "      self.word2count.pop(word)\n",
        "    \n",
        "    def length(self):\n",
        "      return len(self.index2word)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K73GLvsmVK9"
      },
      "source": [
        "def prepareVocab(df):\n",
        "  vocab = Vocab(\"trainingVocab\")\n",
        "  for title in df[\"storytitle\"]:\n",
        "    vocab.addTitle(title)\n",
        "  for s in df[\"sentence1\"]:\n",
        "    vocab.addSentence(s)\n",
        "  for s in df[\"sentence2\"]:\n",
        "    vocab.addSentence(s)\n",
        "  for s in df[\"sentence3\"]:\n",
        "    vocab.addSentence(s)\n",
        "  for s in df[\"sentence4\"]:\n",
        "    vocab.addSentence(s)\n",
        "  for s in df[\"sentence5\"]:\n",
        "    vocab.addSentence(s)\n",
        "  for storyline in df[\"train_storylines\"]:\n",
        "    vocab.addStoryline(storyline)\n",
        "  \n",
        "  #go thru counts and combine all those with just 1 count into <unk>\n",
        "  #unkCount = 0\n",
        "  #low_dict = {key: value for key, value in vocab.word2count.items() if value == 1}\n",
        "  #for key, value in low_dict.items():\n",
        "    #add 1 to unkCount and remove from dict\n",
        "    #unkCount += 1\n",
        "    #vocab.removeWordFromCount(key)\n",
        "  # add unk, then add rest of words to dicts\n",
        "  #vocab.addWordToDicts(\"<unk>\")\n",
        "  for key, value in vocab.word2count.items():\n",
        "    vocab.addWordToDicts(key)\n",
        "  \n",
        "  return vocab\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uBOv8iNxTX5"
      },
      "source": [
        "# divide into training, (dev and test)\n",
        "msk = np.random.rand(len(df)) < 0.01 #reducing training just to test something out (0.8)\n",
        "reduced_df = df[msk]\n",
        "msk = np.random.rand(len(reduced_df)) < 0.8\n",
        "train_df = reduced_df[msk]\n",
        "eval_df = reduced_df[~msk]\n",
        "msk = np.random.rand(len(eval_df)) < 0.5\n",
        "dev_df = eval_df[msk]\n",
        "test_df = eval_df[~msk]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW13EF7awgxq"
      },
      "source": [
        "#prepare vocab! Put both training and test?\n",
        "vocab = prepareVocab(reduced_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wbWSGxv_eth"
      },
      "source": [
        "INPUT_SIZE = 15\n",
        "HIDDEN_SIZE = 1000\n",
        "EMBEDDING_SIZE = 500\n",
        "OUTPUT_SIZE = vocab.length()\n",
        "TARGET_SIZE = 5\n",
        "NUM_LAYERS = 1\n",
        "BATCH_SIZE = 50\n",
        "NUM_WORKERS = 0\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKpwslcwzO7m"
      },
      "source": [
        "# create datasets \n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "class StorylineDataset (Dataset):\n",
        "    def __init__(self, titles, storylines, vocab, input_size):\n",
        "        #must convert words to numeric representations\n",
        "        self.titles = []\n",
        "        for title in titles:\n",
        "          temp = []\n",
        "          for word in title.split():\n",
        "            if word in vocab.word2index:\n",
        "              temp.append(vocab.word2index[word])\n",
        "            else:\n",
        "              temp.append(vocab.word2index[\"\"])\n",
        "          #pad\n",
        "          self.titles.append(np.pad(temp, (input_size-len(temp), 0), 'constant'))\n",
        "        \n",
        "        self.storylines = []\n",
        "        for storyline in storylines:\n",
        "          temp = []\n",
        "          for word in storyline:\n",
        "            if word in vocab.word2index:\n",
        "              temp.append(vocab.word2index[word])\n",
        "            else:\n",
        "              temp.append(vocab.word2index[\"\"])\n",
        "          self.storylines.append(temp)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.titles)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        title = np.asarray(self.titles[index]) \n",
        "        storyline = np.asarray(self.storylines[index])\n",
        "\n",
        "        title = torch.from_numpy(title).long()\n",
        "        storyline = torch.from_numpy(storyline).long()   \n",
        "        return (title, storyline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv6DNXzX03Lr"
      },
      "source": [
        "training_data = StorylineDataset(train_df[\"storytitle\"], train_df[\"train_storylines\"], vocab, INPUT_SIZE)\n",
        "testing_data = StorylineDataset(test_df[\"storytitle\"], test_df[\"train_storylines\"], vocab, INPUT_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dYml2oOC0t9"
      },
      "source": [
        "training_loader = torch.utils.data.DataLoader(training_data, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
        "testing_loader = torch.utils.data.DataLoader(testing_data, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V2TnOxBjJMQ"
      },
      "source": [
        "#encoder\n",
        "class TitleEncoder(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, embedding_size, vocab_size, num_layers):\n",
        "        super(TitleEncoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_size = input_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        #print(vocab_size)\n",
        "        #print(embedding_size)\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size)\n",
        "        self.bilstm = torch.nn.LSTM(self.embedding_size, self.hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
        "        self.hidden2hidden = torch.nn.Linear(2*self.hidden_size, self.hidden_size)\n",
        "\n",
        "    def forward(self, title):\n",
        "        embedded = self.embedding(title)\n",
        "        bilstm_out, hidden_out = self.bilstm(embedded)\n",
        "        #print(hidden_out)\n",
        "        #print(type(hidden_out))\n",
        "        output = self.hidden2hidden(bilstm_out)\n",
        "        return output, hidden_out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXywYlbwnAs8"
      },
      "source": [
        "#decoder\n",
        "# need to incorporate attention: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "#initial input token is the start of string token, and the first hidden state si the context vector\n",
        "class StorylineDecoder(torch.nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, embedding_size, num_layers, e_dropout_p, h_dropout_p=0.1):\n",
        "        super(StorylineDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.output_size = output_size\n",
        "        self.e_dropout_p = e_dropout_p\n",
        "        self.h_dropout_p = h_dropout_p\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(self.output_size, self.embedding_size)\n",
        "        self.lstm = torch.nn.LSTM(self.embedding_size, self.hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.out = torch.nn.Linear(self.hidden_size, self.output_size)\n",
        "        #self.softmax = torch.nn.LogSoftmax(dim=1)\n",
        "        self.e_dropout = torch.nn.Dropout(self.e_dropout_p)\n",
        "        self.h_dropout = torch.nn.Dropout(self.h_dropout_p)       \n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input)\n",
        "        embedded = self.e_dropout(embedded)\n",
        "        lstm_out, hidden_out = self.lstm(embedded, hidden)\n",
        "        output = self.out(lstm_out)\n",
        "        output = self.h_dropout(output)\n",
        "        #output = self.softmax(output)\n",
        "        return output, hidden_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-00KfJbADJFm"
      },
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, batch_size=BATCH_SIZE, target_length=TARGET_SIZE):\n",
        "    encoder_hidden = torch.zeros(1, 1, HIDDEN_SIZE, device=device)\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = INPUT_SIZE#input_tensor.size(0)\n",
        "    #target_length = TARGET_SIZE#target_tensor.size(0)\n",
        "\n",
        "    #encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    #batch first\n",
        "    encoder_output, hidden = encoder(input_tensor)\n",
        "    #encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    #start with SOS token\n",
        "    decoder_input = torch.tensor([[0]], device=device)\n",
        "\n",
        "\n",
        "    use_teacher_forcing = False #True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        #go thru batch first\n",
        "        for b in range(len(hidden[0][0])):\n",
        "          h_0 = torch.zeros(1, 1, HIDDEN_SIZE*2, device=device)\n",
        "          h_0[0][0] = hidden[0].transpose(0,1)[b].flatten()\n",
        "          c_0 = torch.zeros(1, 1, HIDDEN_SIZE*2, device=device)\n",
        "          c_0[0][0] = hidden[1].transpose(0,1)[b].flatten()\n",
        "          decoder_hidden = (h_0, c_0)\n",
        "\n",
        "          for di in range(target_length):\n",
        "              decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "              topv, topi = decoder_output.topk(1)\n",
        "              decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "              decoder_input = torch.tensor([[decoder_input]], device=device)\n",
        "              #print(target_tensor)\n",
        "              reshaped_target = torch.zeros(1, device=device).long()\n",
        "              reshaped_target[0] = target_tensor[b][di]\n",
        "              #print(target_tensor[b][di])\n",
        "              #print(reshaped_target)\n",
        "              #print(vocab.index2word[topi[0][0][0].item()])\n",
        "              #print(vocab.index2word[reshaped_target[0].item()])\n",
        "              #print(decoder_output)\n",
        "              #print(decoder_output[0])\n",
        "              loss += criterion(decoder_output[0], reshaped_target)\n",
        "\n",
        "    #print(\"backwards step\")\n",
        "    loss.backward()\n",
        "    print(\"loss: \", loss.item() / (len(hidden[0][0])*target_length))\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / (len(hidden[0][0])*target_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUsCgJ6MDHx_"
      },
      "source": [
        "#init models\n",
        "encoder_model = TitleEncoder(INPUT_SIZE, HIDDEN_SIZE, EMBEDDING_SIZE, vocab.length(), NUM_LAYERS).to(device)\n",
        "decoder_model = StorylineDecoder(HIDDEN_SIZE*2, OUTPUT_SIZE, EMBEDDING_SIZE, NUM_LAYERS, 0.4).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXwAvRu8iS3V"
      },
      "source": [
        "#load encoder_model\n",
        "encoder_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/encoder_title.pt\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mMpfiJiiTSG"
      },
      "source": [
        "#load decoder_model\n",
        "decoder_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/decoder_storyline.pt\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taT3f4UG-QmD"
      },
      "source": [
        "# create criterion and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "encoder_optimizer = torch.optim.SGD(encoder_model.parameters(), lr = 0.01, momentum=0.9)#torch.optim.Adam(encoder_model.parameters(), lr=0.01) #torch.optim.SGD(encoder_model.parameters(), lr = 0.01, momentum=0.9)\n",
        "decoder_optimizer = torch.optim.SGD(decoder_model.parameters(), lr = 0.01, momentum=0.9)#torch.optim.Adam(decoder_model.parameters(), lr=0.01) #torch.optim.SGD(decoder_model.parameters(), lr = 0.01, momentum=0.9)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEfEGxTjyZbY"
      },
      "source": [
        "def trainModel(num_epochs, train_loader, test_loader, encoder_model, decoder_model, encoder_optimizer, decoder_optimizer, criterion):\n",
        "  #keep track of validation loss\n",
        "  valid_loss_min = float('inf')\n",
        "  for epoch in range(1, num_epochs+1):\n",
        "            total_loss = 0.0\n",
        "            #put models in train mode\n",
        "            encoder_model.train()\n",
        "            decoder_model.train()\n",
        "            for titles, storylines in train_loader:\n",
        "              #send to cuda\n",
        "              titles.to(device)\n",
        "              storylines.to(device)\n",
        "              #call train function\n",
        "              total_loss += train(titles, storylines, encoder_model, decoder_model, encoder_optimizer, decoder_optimizer, criterion)\n",
        "            \n",
        "            print(\"Epoch \", epoch, \" training loss: \", total_loss/len(train_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olFgs3p_ESS8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0004cfb-98e9-4d5b-aa3d-dac8581d777f"
      },
      "source": [
        "# call trainModel 0.047\n",
        "trainModel(3, training_loader, testing_loader, encoder_model, decoder_model, encoder_optimizer, decoder_optimizer, criterion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  20.149923828125\n",
            "loss:  24.896830078125\n",
            "loss:  17.19998828125\n",
            "loss:  25.257783203125\n",
            "loss:  25.016263671875\n",
            "loss:  24.500158203125\n",
            "loss:  20.377439453125\n",
            "loss:  27.589599609375\n",
            "loss:  19.265682547433034\n",
            "Epoch  1  training loss:  22.694852097284226\n",
            "loss:  17.20325390625\n",
            "loss:  19.303689453125\n",
            "loss:  16.496734375\n",
            "loss:  19.777203125\n",
            "loss:  18.242998046875\n",
            "loss:  25.30678125\n",
            "loss:  18.636916015625\n",
            "loss:  20.6531484375\n",
            "loss:  21.169011579241072\n",
            "Epoch  2  training loss:  19.64330402095734\n",
            "loss:  17.0391640625\n",
            "loss:  16.0817314453125\n",
            "loss:  12.9033359375\n",
            "loss:  15.1892333984375\n",
            "loss:  16.36453515625\n",
            "loss:  14.4716396484375\n",
            "loss:  16.3831845703125\n",
            "loss:  13.1266572265625\n",
            "loss:  12.277472795758928\n",
            "Epoch  3  training loss:  14.87077269345238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1de13V5hWMj"
      },
      "source": [
        "#save encoder_model\n",
        "torch.save(encoder_model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/encoder_title.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTpnO_o5hW7q"
      },
      "source": [
        "#save decoder_model\n",
        "torch.save(encoder_model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/decoder_storyline.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZPQVVAOrrOC"
      },
      "source": [
        "def evaluate(input_tensor, target_tensor, encoder, decoder, criterion, batch_size=BATCH_SIZE, target_length=TARGET_SIZE):\n",
        "  with torch.no_grad():\n",
        "    encoder_hidden = torch.zeros(1, 1, HIDDEN_SIZE, device=device)\n",
        "\n",
        "    input_length = INPUT_SIZE#input_tensor.size(0)\n",
        "    #target_length = TARGET_SIZE#target_tensor.size(0)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    #batch first\n",
        "    encoder_output, hidden = encoder(input_tensor)\n",
        "    #encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    #start with SOS token\n",
        "    decoder_input = torch.tensor([[0]], device=device)\n",
        "\n",
        "    titles = []\n",
        "    predicteds = []\n",
        "    targets = []\n",
        "\n",
        "\n",
        "    # Without teacher forcing: use its own predictions as the next input\n",
        "    #go thru batch first\n",
        "    for b in range(len(hidden[0][0])):\n",
        "      title = []\n",
        "      predicted = []\n",
        "      target = []\n",
        "      for i in range(len(input_tensor[b])):\n",
        "        title.append(vocab.index2word[input_tensor[b][i].item()])\n",
        "\n",
        "      h_0 = torch.zeros(1, 1, HIDDEN_SIZE*2, device=device)\n",
        "      h_0[0][0] = hidden[0].transpose(0,1)[b].flatten()\n",
        "      c_0 = torch.zeros(1, 1, HIDDEN_SIZE*2, device=device)\n",
        "      c_0[0][0] = hidden[1].transpose(0,1)[b].flatten()\n",
        "      decoder_hidden = (h_0, c_0)\n",
        "      \n",
        "      for di in range(target_length):\n",
        "          decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "          topv, topi = decoder_output.topk(1)\n",
        "          decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "          decoder_input = torch.tensor([[decoder_input]], device=device)\n",
        "          #print(target_tensor)\n",
        "          reshaped_target = torch.zeros(1, device=device).long()\n",
        "          reshaped_target[0] = target_tensor[b][di]\n",
        "          loss += criterion(decoder_output[0], reshaped_target)\n",
        "\n",
        "          predicted.append(vocab.index2word[topi[0][0][0].item()])\n",
        "          target.append(vocab.index2word[reshaped_target[0].item()])\n",
        "          \n",
        "      if (b == 0 or b == len(hidden[0][0])-1):\n",
        "        print(\"title: \", title)\n",
        "        print(\"predicted: \", predicted)\n",
        "        print(\"Target: \", target)\n",
        "        print('\\n')\n",
        "      #append to output lists\n",
        "      titles.append(title)\n",
        "      predicteds.append(predicted)\n",
        "      targets.append(target)\n",
        "\n",
        "    return (loss.item() / (len(hidden[0][0])*target_length), titles, predicteds, targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54yAtDFuqMYe"
      },
      "source": [
        "def evaluateModel(test_loader, encoder_model, decoder_model, criterion):\n",
        "  #keep track of validation loss\n",
        "  valid_loss_min = float('inf')\n",
        "  total_loss = 0.0\n",
        "  for titles, storylines in test_loader:\n",
        "    #send to cuda\n",
        "    titles.to(device)\n",
        "    storylines.to(device)\n",
        "    #call train function\n",
        "    loss, titles, predicteds, targets = evaluate(titles, storylines, encoder_model, decoder_model, criterion)\n",
        "    total_loss += loss\n",
        "  print(\"total loss: \", total_loss/len(test_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKoVtRwjrfuN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fa4bdd1-efa3-42be-cf89-d45a7e982ea6"
      },
      "source": [
        "evaluateModel(testing_loader, encoder_model, decoder_model, criterion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "title:  ['<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', 'Mary', \"isn't\", 'home']\n",
            "predicted:  ['party', 'wanted', 'home', 'party', 'car']\n",
            "Target:  ['mary', 'mall', 'running', 'pulled', 'knocks']\n",
            "\n",
            "\n",
            "title:  ['<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', 'No', 'Help']\n",
            "predicted:  ['party', 'car', 'go', 'car', 'unfortunately']\n",
            "Target:  ['solving', 'help', 'wanted', 'hour', 'solved']\n",
            "\n",
            "\n",
            "title:  ['<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', 'The', 'Long', 'Jump']\n",
            "predicted:  ['party', 'car', 'go', 'car', 'unfortunately']\n",
            "Target:  ['jose', 'ready', 'began', 'track', 'crashed']\n",
            "\n",
            "\n",
            "title:  ['<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', 'The', 'Wrong', 'Brand']\n",
            "predicted:  ['party', 'go', 'car', 'unfortunately', 'unfortunately']\n",
            "Target:  ['nate', 'buying', 'upset', 'name', 'nate']\n",
            "\n",
            "\n",
            "total loss:  14.117823079427083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vud9OwECCaR-"
      },
      "source": [
        "# create datasets \n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "class SentencesDataset (Dataset):\n",
        "    def __init__(self, titles, storylines, sentence1, sentence2, sentence3, sentence4, sentence5, vocab, input_size, output_size):\n",
        "        #must convert words to numeric representations\n",
        "        #convert all into separate arrays first\n",
        "        self.titles = []\n",
        "        for title in titles:\n",
        "          temp = []\n",
        "          for word in title.split():\n",
        "            if word in vocab.word2index:\n",
        "              temp.append(vocab.word2index[word])\n",
        "            else:\n",
        "              temp.append(vocab.word2index[\"\"])\n",
        "          #pad\n",
        "          self.titles.append(temp)\n",
        "        \n",
        "        self.storylines = []\n",
        "        for storyline in storylines:\n",
        "          temp = []\n",
        "          for word in storyline:\n",
        "            if word in vocab.word2index:\n",
        "              temp.append(vocab.word2index[word])\n",
        "            else:\n",
        "              temp.append(vocab.word2index[\"\"])\n",
        "          self.storylines.append(temp)\n",
        "\n",
        "        #convert titles and storylines to combined_input\n",
        "        self.combined_input = []\n",
        "        for i in range(len(self.titles)):\n",
        "          temp = []\n",
        "          temp.extend(self.titles[i])\n",
        "          temp.append(vocab.word2index['<EOT>'])\n",
        "          temp.extend(self.storylines[i])\n",
        "          #pad to input_size amount\n",
        "          self.combined_input.append(np.pad(temp, (input_size-len(temp), 0), 'constant'))\n",
        "\n",
        "        self.sentences = []\n",
        "        for i in range(len(sentence1)):\n",
        "          temp = []\n",
        "          for word in sentence1.iloc[i].split():\n",
        "            if word in vocab.word2index:\n",
        "              temp.append(vocab.word2index[word])\n",
        "            else:\n",
        "              temp.append(vocab.word2index[\"\"])\n",
        "          temp.append(vocab.word2index[\"<EOS>\"])\n",
        "          for word in sentence2.iloc[i].split():\n",
        "            if word in vocab.word2index:\n",
        "              temp.append(vocab.word2index[word])\n",
        "            else:\n",
        "              temp.append(vocab.word2index[\"\"])\n",
        "          temp.append(vocab.word2index[\"<EOS>\"])\n",
        "          for word in sentence3.iloc[i].split():\n",
        "            if word in vocab.word2index:\n",
        "              temp.append(vocab.word2index[word])\n",
        "            else:\n",
        "              temp.append(vocab.word2index[\"\"])\n",
        "          temp.append(vocab.word2index[\"<EOS>\"])\n",
        "          for word in sentence4.iloc[i].split():\n",
        "            if word in vocab.word2index:\n",
        "              temp.append(vocab.word2index[word])\n",
        "            else:\n",
        "              temp.append(vocab.word2index[\"\"])\n",
        "          temp.append(vocab.word2index[\"<EOS>\"])\n",
        "          for word in sentence5.iloc[i].split():\n",
        "            if word in vocab.word2index:\n",
        "              temp.append(vocab.word2index[word])\n",
        "            else:\n",
        "              temp.append(vocab.word2index[\"\"])\n",
        "          temp.append(vocab.word2index[\"<EOS>\"])\n",
        "          #append full story to sentences, making sure to pad\n",
        "          self.sentences.append(np.pad(temp, (0, output_size-len(temp)), 'constant'))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.titles)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        title = np.asarray(self.titles[index]) \n",
        "        storyline = np.asarray(self.storylines[index])\n",
        "        input = np.asarray(self.combined_input[index])\n",
        "        story = np.asarray(self.sentences[index])\n",
        "\n",
        "        #title = torch.from_numpy(title).long()\n",
        "        #storyline = torch.from_numpy(storyline).long()\n",
        "        input = torch.from_numpy(input).long()   \n",
        "        story = torch.from_numpy(story).long()\n",
        "        return (input, story)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0wMGT8kSwRe"
      },
      "source": [
        "COMBINED_INPUT_SIZE = 20\n",
        "STORY_OUTPUT_SIZE = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-2jyCYHR7Zr"
      },
      "source": [
        "training_data_story = SentencesDataset(train_df[\"storytitle\"], train_df[\"train_storylines\"], train_df[\"sentence1\"], train_df[\"sentence2\"], train_df[\"sentence3\"], train_df[\"sentence4\"], train_df[\"sentence5\"], vocab, COMBINED_INPUT_SIZE, STORY_OUTPUT_SIZE)\n",
        "testing_data_story = SentencesDataset(test_df[\"storytitle\"], test_df[\"train_storylines\"], test_df[\"sentence1\"], test_df[\"sentence2\"], test_df[\"sentence3\"], test_df[\"sentence4\"], test_df[\"sentence5\"], vocab, COMBINED_INPUT_SIZE, STORY_OUTPUT_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJBBIvzBR8y_"
      },
      "source": [
        "training_loader_story = torch.utils.data.DataLoader(training_data_story, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
        "testing_loader_story = torch.utils.data.DataLoader(testing_data_story, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFt4Ot4cU6h3"
      },
      "source": [
        "#can we use previous model functions with different parameters?\n",
        "#init models\n",
        "encoder_model_story = TitleEncoder(COMBINED_INPUT_SIZE, HIDDEN_SIZE, EMBEDDING_SIZE, vocab.length(), NUM_LAYERS).to(device)\n",
        "decoder_model_story = StorylineDecoder(HIDDEN_SIZE*2, OUTPUT_SIZE, EMBEDDING_SIZE, NUM_LAYERS, 0.2).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MtsZL60VaOY"
      },
      "source": [
        "# create criterion and optimizer\n",
        "criterion_story = torch.nn.CrossEntropyLoss() #torch.nn.NLLLoss()\n",
        "encoder_optimizer_story = torch.optim.SGD(encoder_model.parameters(), lr = 0.01, momentum=0.9)\n",
        "decoder_optimizer_story = torch.optim.SGD(decoder_model.parameters(), lr = 0.01, momentum=0.9)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kci-EePlVADX"
      },
      "source": [
        "def trainModelStory(num_epochs, train_loader, test_loader, encoder_model, decoder_model, encoder_optimizer, decoder_optimizer, criterion):\n",
        "  #keep track of validation loss\n",
        "  valid_loss_min = float('inf')\n",
        "  for epoch in range(1, num_epochs+1):\n",
        "            total_loss = 0.0\n",
        "            #put models in train mode\n",
        "            encoder_model.train()\n",
        "            decoder_model.train()\n",
        "            for inputs, stories in train_loader:\n",
        "              #send to cuda\n",
        "              inputs.to(device)\n",
        "              stories.to(device)\n",
        "              #call train function\n",
        "              total_loss += train(inputs, stories, encoder_model, decoder_model, encoder_optimizer, decoder_optimizer, criterion, target_length=STORY_OUTPUT_SIZE)\n",
        "              #break #for testing purposes only\n",
        "            print(\"Epoch \", epoch, \" training loss: \", total_loss/len(train_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "JcR74Ztkeuja",
        "outputId": "84e6c077-72a0-40b7-d8f5-7ab52c43e05d"
      },
      "source": [
        "# call trainModel\n",
        "print(vocab.length())\n",
        "trainModelStory(1, training_loader_story, testing_loader_story, encoder_model_story, decoder_model_story, encoder_optimizer_story, decoder_optimizer_story, criterion_story)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5643\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-0c4ef446c22e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# call trainModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainModelStory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loader_story\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_loader_story\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_model_story\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_model_story\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer_story\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer_story\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_story\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-55-674afced3e4e>\u001b[0m in \u001b[0;36mtrainModelStory\u001b[0;34m(num_epochs, train_loader, test_loader, encoder_model, decoder_model, encoder_optimizer, decoder_optimizer, criterion)\u001b[0m\n\u001b[1;32m     12\u001b[0m               \u001b[0mstories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m               \u001b[0;31m#call train function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m               \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTORY_OUTPUT_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m               \u001b[0;31m#break #for testing purposes only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" training loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-266d1cff4a99>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, batch_size, target_length)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m#print(\"backwards step\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtarget_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hLA4S-sBYRz"
      },
      "source": [
        "#save encoder_model\n",
        "torch.save(encoder_model_story.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/encoder_combined.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HO9aH0C8Be2-"
      },
      "source": [
        "#save decoder_model\n",
        "torch.save(decoder_model_story.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/decoder_story.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzlnOd9jNxbu"
      },
      "source": [
        "def evaluateModelStory(test_loader, encoder_model, decoder_model, criterion):\n",
        "  #keep track of validation loss\n",
        "  valid_loss_min = float('inf')\n",
        "  total_loss = 0.0\n",
        "  for titles, storylines in test_loader:\n",
        "    #send to cuda\n",
        "    titles.to(device)\n",
        "    storylines.to(device)\n",
        "    #call train function\n",
        "    loss, titles, predicteds, targets = evaluate(titles, storylines, encoder_model, decoder_model, criterion, target_length=STORY_OUTPUT_SIZE)\n",
        "    total_loss += loss\n",
        "  print(\"total loss: \", total_loss/len(test_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YWv9QR3PLAZ",
        "outputId": "1583294b-e1fc-470a-829a-0c85814ac6dd"
      },
      "source": [
        "evaluateModelStory(testing_loader_story, encoder_model_story, decoder_model_story, criterion_story)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "title:  ['<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', 'The', 'Solo', '<EOT>', 'solo', 'blew', 'time', 'gave', 'took']\n",
            "predicted:  ['divorce', 'house!', 'house!', 'push.', 'negotiating', 'wetsuit.', 'bleachers.', \"world's\", \"billy's\", 'Margaret', 'following', 'application', 'old.', 'Tally', \"He's\", 'ecology', 'Whopper', 'wall,', 'theater.', 'test,', 'stomach', 'Thieving', 'water', 'sports', 'argument.', 'like', 'rather', 'rather', 'stock', 'horseshoe', 'should', 'removed,', 'fussed', 'time', 'glass.', 'names', 'names', 'cheating', 'names', 'names', 'cheating', 'emailed', 'numbers', 'restaurant.', 'hoop.', 'hoop.', 'Afterwards,', 'aimlessly', 'deb', 'knit', 'he', 'Student', 'sad.', 'Tea', 'countless', 'declined.', \"Year's\", 'more', 'before', 'spicy.', 'steps', 'presented', 'too.', 'within', 'paint', 'gas', 'match', 'humvee', 'insisted', 'realtor.', 'yet', 'King.', 'lugged', 'parlor.', 'river,', 'tom', 'parents,', 'Table', 'jar', 'mini', 'Once', 'treating.', 'howling', 'Clothes', 'paycheck.', 'Sylvie', 'new.', 'Kirby', 'actually', 'plate', 'steps', 'tickets.', 'boat', 'hear', 'light.', 'Emptying', 'want', 'negotiating', 'begged', 'wetsuit.']\n",
            "Target:  ['Anthony', 'auditioned', 'for', 'a', 'solo.', '<EOS>', 'He', 'blew', 'the', 'judges', 'away.', '<EOS>', 'When', 'it', 'was', 'time', 'to', 'perform,', 'Anthony', 'got', 'cold', 'feet.', '<EOS>', 'He', 'gave', 'himself', 'a', 'pep', 'talk.', '<EOS>', 'Finally,', 'he', 'took', 'the', 'stage', 'and', 'sang.', '<EOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>']\n",
            "\n",
            "\n",
            "title:  ['<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', \"Surf's\", 'Up!', '<EOT>', 'went', 'wetsuit', 'water', 'waited', 'shore']\n",
            "predicted:  ['mad,', 'arrested', 'cross-country', 'Whopper', 'cross-country', 'Whopper', 'wall,', 'theater.', 'test,', 'stomach', 'Thieving', 'water', 'sports', 'argument.', 'like', 'rather', 'rather', 'stock', 'horseshoe', 'should', 'removed,', 'fussed', 'time', 'glass.', 'names', 'names', 'cheating', 'names', 'names', 'cheating', 'emailed', 'numbers', 'restaurant.', 'hoop.', 'hoop.', 'Afterwards,', 'aimlessly', 'deb', 'knit', 'he', 'Student', 'sad.', 'Tea', 'countless', 'declined.', \"Year's\", 'more', 'before', 'spicy.', 'steps', 'presented', 'too.', 'within', 'paint', 'gas', 'match', 'humvee', 'insisted', 'realtor.', 'yet', 'King.', 'lugged', 'parlor.', 'river,', 'tom', 'parents,', 'Table', 'jar', 'mini', 'Once', 'treating.', 'howling', 'Clothes', 'paycheck.', 'Sylvie', 'new.', 'Kirby', 'actually', 'plate', 'steps', 'tickets.', 'boat', 'hear', 'light.', 'Emptying', 'want', 'negotiating', 'begged', 'wetsuit.', 'role', 'draft', 'role!', 'bite', 'ingredients', 'tyler', 'determined', 'parlor.', 'turn,', 'virus', 'kickboxer.']\n",
            "Target:  ['Last', 'weekend', 'I', 'grabbed', 'my', 'surfboard', 'and', 'went', 'down', 'to', 'the', 'beach.', '<EOS>', 'It', 'was', 'really', 'cold', 'out', 'so', 'I', 'had', 'to', 'wear', 'my', 'wetsuit.', '<EOS>', 'When', 'I', 'got', 'into', 'the', 'water', 'I', 'realized', 'that', 'it', 'was', 'warmer', 'than', 'the', 'air.', '<EOS>', 'I', 'waited', 'a', 'long', 'time', 'for', 'the', 'perfect', 'wave.', '<EOS>', 'The', 'perfect', 'wave', 'came', 'towards', 'me', 'and', 'I', 'rode', 'it', 'happily', 'to', 'shore.', '<EOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>']\n",
            "\n",
            "\n",
            "title:  ['<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', 'Blind', 'Date', '<EOT>', 'tim', 'blind', 'tim', 'went', 'long']\n",
            "predicted:  ['principle', \"Matt's\", 'xbox', 'am', 'tasted', 'disabled', 'amazing', 'parlor.', 'trips', 'morning,', 'morning,', 'morning,', 'skate', 'Ota', 'stay', 'tricks', 'turned', 'coma.', 'stay', 'Alabama', 'quiet,', \"Year's\", 'hungry.', 'countless', 'support', 'Much', 'quiet,', 'bumped', 'days', 'friend', 'too.', 'taking', 'Return', 'Tonight', 'changes', 'though,', 'excitedly', 'Whopper', 'cross-country', 'Whopper', 'wall,', 'theater.', 'test,', 'stomach', 'Thieving', 'water', 'sports', 'argument.', 'like', 'rather', 'rather', 'stock', 'horseshoe', 'should', 'removed,', 'fussed', 'time', 'glass.', 'names', 'names', 'cheating', 'names', 'names', 'cheating', 'emailed', 'numbers', 'restaurant.', 'hoop.', 'hoop.', 'Afterwards,', 'aimlessly', 'deb', 'knit', 'he', 'Student', 'sad.', 'Tea', 'countless', 'declined.', \"Year's\", 'more', 'before', 'spicy.', 'steps', 'presented', 'too.', 'within', 'paint', 'gas', 'match', 'humvee', 'insisted', 'realtor.', 'yet', 'King.', 'lugged', 'parlor.', 'river,', 'tom', 'parents,']\n",
            "Target:  ['Tim', 'had', 'been', 'alone', 'for', 'a', 'while.', '<EOS>', 'His', 'friends', 'set', 'him', 'up', 'on', 'a', 'blind', 'date.', '<EOS>', 'Tim', 'was', 'nervous', 'and', 'awkward', 'throughout.', '<EOS>', 'The', 'date', 'went', 'horribly', 'wrong.', '<EOS>', 'Tim', 'remained', 'single', 'for', 'a', 'long', 'time', 'after.', '<EOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>']\n",
            "\n",
            "\n",
            "title:  ['<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', 'amnesia', '<EOT>', 'remembering', 'belongings', 'car', 'find', 'things']\n",
            "predicted:  ['Cream', 'playing.', 'release', 'Hard', 'spare.', 'sound.', 'banning', 'further.', 'monkey.', 'carton', 'carton', 'carton', 'carton', 'grabbed', \"They'd\", 'exam.', 'carton', 'carton', 'carton', 'carton', 'grabbed', \"They'd\", 'exam.', 'carton', 'carton', 'carton', 'carton', 'grabbed', \"They'd\", 'exam.', 'carton', 'carton', 'carton', 'carton', 'grabbed', \"They'd\", 'exam.', 'carton', 'carton', 'carton', 'carton', 'grabbed', \"They'd\", 'exam.', 'carton', 'carton', 'carton', 'carton', 'grabbed', \"They'd\", 'exam.', 'carton', 'carton', 'carton', 'carton', 'grabbed', \"They'd\", 'exam.', 'carton', 'carton', 'carton', 'carton', 'grabbed', \"They'd\", 'exam.', 'carton', 'carton', 'carton', 'carton', 'grabbed', \"They'd\", 'exam.', 'carton', 'carton', 'carton', 'carton', 'grabbed', \"They'd\", 'exam.', 'carton', 'carton', 'carton', 'carton', 'grabbed', \"They'd\", 'exam.', 'carton', 'carton', 'carton', 'carton', 'grabbed', \"They'd\", 'exam.', 'carton', 'carton', 'carton', 'carton', 'grabbed', \"They'd\", 'exam.']\n",
            "Target:  ['John', 'woke', 'up', 'on', 'the', 'beach', 'not', 'remembering', 'who', 'he', 'was.', '<EOS>', 'John', 'looked', 'at', 'his', 'belongings.', '<EOS>', 'John', 'found', 'a', 'car', 'key.', '<EOS>', 'John', 'used', 'the', 'car', 'key', 'to', 'find', 'out', 'the', 'car.', '<EOS>', 'John', 'remembered', 'who', 'he', 'was', 'from', 'his', 'things', 'in', 'the', 'car.', '<EOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>', '<SOS>']\n",
            "\n",
            "\n",
            "total loss:  8.668418261718749\n"
          ]
        }
      ]
    }
  ]
}